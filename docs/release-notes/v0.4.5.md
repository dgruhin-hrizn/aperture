# Aperture v0.4.5 Release Notes

Hey everyone! üëã

This release focuses on **Ollama compatibility improvements** and a refined model selection experience. If you've been having trouble getting local LLMs to work with Aperture's chat assistant, this update is for you!

---

## ü¶ô Ollama Compatibility Fixes

### Tool Argument Normalization

Local LLMs like Llama and Qwen have quirks when calling tools that cloud models don't:

- **Null values** ‚Äî Local models send `null` for optional parameters instead of omitting them
- **String numbers** ‚Äî Send `"15"` instead of `15`
- **String booleans** ‚Äî Send `"true"` instead of `true`

Aperture now automatically normalizes these before validation, fixing errors like:

```
Invalid input for tool searchByKeyword: Expected number, received string
```

All 21 tool schemas are now wrapped with `nullSafe()` preprocessing.

### Curated Model List

We've refined the Ollama model recommendations based on real-world testing:

**Chat Models (with tool calling):**
- **Qwen 3** ‚Äî Recommended. Excellent tool calling and reasoning.
- **FireFunction v2** ‚Äî Specialized for function calling. GPT-4 level tools.

**Text Generation Models:**
- **Llama 3.2** ‚Äî Recommended. Latest Llama, fast and capable.
- **Llama 3.1** ‚Äî Proven and reliable. 8B/70B/405B sizes available.
- **Gemma 3** ‚Äî Fast and efficient. Great for limited hardware.
- **Phi 4** ‚Äî Compact 14B model. Punches above its weight.

**Embedding Models:**
- **Nomic Embed Text** ‚Äî Recommended. Good quality 768-dim embeddings.
- **mxbai Embed Large** ‚Äî Higher quality 1024-dim embeddings.
- **All MiniLM** ‚Äî Fast but lower quality. 384-dim embeddings.
- **Nomic Embed Text v2 MoE** ‚Äî Best for non-English content.

**Removed from chat:** Llama 3.2, Llama 3.1, Mistral, Qwen 2.5 (unreliable tool calling)

---

## üéõÔ∏è Custom Model Support

Have powerful hardware? Want to run `llama3.3:70b` or `mixtral:8x22b`?

For **Ollama** and **OpenAI-Compatible** providers, you can now select **"Custom Model..."** from the dropdown and enter any model name manually.

- Text input appears with helpful placeholders
- Points to ollama.com/library for model names
- Works with Test and Save just like predefined models
- Previously saved custom models load correctly

---

## üé® UI Improvements

### AI Configuration Cards

- **Stacked layout** ‚Äî Provider and model dropdowns now on separate lines for better readability
- **Improved Ollama instructions** ‚Äî Card-based design with note chips (recommended, fast, multilingual)
- **Responsive chips** ‚Äî Wrap to new line on narrow screens instead of getting cut off
- **Complete model coverage** ‚Äî All dropdown options now shown in pull instructions

### Model Descriptions

Updated all Ollama model descriptions to be clearer and more helpful:

- Include embedding dimensions where relevant
- Indicate recommended choices
- Highlight key characteristics (fast, compact, multilingual)

---

## üîß Technical Details

### New Utility: `nullSafe()`

```typescript
// apps/api/src/routes/assistant/tools/utils.ts

export function nullSafe<T extends z.ZodTypeAny>(schema: T) {
  return z.preprocess(normalizeToolArgs, schema) as unknown as T
}
```

Wraps Zod schemas to:
1. Strip null values (convert to undefined)
2. Coerce string numbers to numbers
3. Coerce string booleans to booleans

### Model List Changes

**Ollama chatModels:** `qwen3`, `firefunction-v2`

**Ollama textGenerationModels:** `llama3.2`, `llama3.1`, `gemma3`, `phi4`

**Ollama embeddingModels:** `nomic-embed-text`, `mxbai-embed-large`, `all-minilm`, `nomic-embed-text-v2-moe`

---

## üöÄ Upgrade Instructions

### For Existing Ollama Users

1. **Re-select your chat model** ‚Äî If you were using Llama 3.2 or Mistral for chat, switch to **Qwen 3** or **FireFunction v2**
2. **Text generation is fine** ‚Äî Llama models work great for text generation, just not tool calling
3. **Test your setup** ‚Äî Use the Test button to verify connectivity

### For New Ollama Users

1. Install recommended models:
   ```bash
   ollama pull nomic-embed-text  # embeddings
   ollama pull qwen3             # chat
   ollama pull llama3.2          # text generation
   ```

2. Configure in **Admin ‚Üí Settings ‚Üí AI / LLM Setup**

3. Test each function before saving

---

## üôè Thanks

Thanks to the community members who tested Ollama integration and reported the tool calling issues. Your detailed error logs helped identify the null/string coercion problems!

---

## üìã Full Changelog

- `fix(ollama): strip null values from tool args for local LLM compatibility`
- `fix(ollama): coerce string numbers and booleans in tool args`
- `chore(ollama): trim chat models to reliable tool-calling options`
- `feat(ollama): add llama3.1 and llama3.2 to text generation models`
- `feat(ollama): set Qwen 3 as recommended chat model`
- `feat(ui): add custom model option for Ollama and OpenAI-compatible`
- `style(ui): stack provider and model dropdowns vertically`
- `style(ui): improve Ollama installation instructions design`
- `docs: update Ollama model descriptions to be clearer`

---

Enjoy the improved Ollama support! ü¶ôüçø

